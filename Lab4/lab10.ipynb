{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB10\n",
    "\n",
    "The goal of this lab is to use Reinforcement Learning to create an agent capable of playing tic tac toe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "from random import choice\n",
    "from math import ceil\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the game\n",
    "We save the positions of the Xs and Os in separate sets and we map the board as a magic-square:\n",
    "\n",
    "| 2     | 9     | 4     |\n",
    "|-------|-------|-------|\n",
    "| **7** | **5** | **3** |\n",
    "| **6** | **1** | **8** |\n",
    "\n",
    "To check if any player has won we check if exists a permutation of 3 cells with their symbol whose sum is 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.x_pos = set()\n",
    "        self.o_pos = set()\n",
    "        self.moves = set(range(1, 10)) #available moves\n",
    "        self.next = 'x'\n",
    "\n",
    "    def play(self, pos): #Returns True if the move was accepted and played, False otherwise\n",
    "        if pos>9 or pos<1 or pos not in self.moves: #square non-existent or already taken\n",
    "            return False\n",
    "        if self.next == 'x':\n",
    "            self.x_pos.add(pos)\n",
    "            self.moves.remove(pos)\n",
    "            self.next = 'o'\n",
    "        else:\n",
    "            self.o_pos.add(pos)\n",
    "            self.moves.remove(pos)\n",
    "            self.next = 'x'\n",
    "        return True\n",
    "    \n",
    "    def check_victory(self):\n",
    "        states = [None, 'x wins', 'o wins', 'invalid state', 'the game was a tie']\n",
    "        res = 0\n",
    "        for t in permutations(self.x_pos, 3):\n",
    "            if sum(t) == 15:\n",
    "                if res==0 or res==1:\n",
    "                    res = 1\n",
    "                else:\n",
    "                    res = 3\n",
    "                break\n",
    "        for t in permutations(self.o_pos, 3):\n",
    "            if sum(t) == 15:\n",
    "                if res==0 or res==2:\n",
    "                    res = 2\n",
    "                else:\n",
    "                    res = 3\n",
    "                break\n",
    "        if res == 0 and not self.moves:\n",
    "            res = 4\n",
    "        return states[res], res\n",
    "    \n",
    "    def board(self):  #shows the board state\n",
    "        coord = {\n",
    "        1: (2, 1),\n",
    "        2: (0, 0),\n",
    "        3: (1 ,2),\n",
    "        4: (0, 2),\n",
    "        5: (1, 1),\n",
    "        6: (2, 0),\n",
    "        7: (1, 0),\n",
    "        8: (2, 2),\n",
    "        9: (0, 1),\n",
    "        }\n",
    "        mat = [[\"-\" for _ in range(3)] for _ in range(3)]\n",
    "        for i in range(1, 10):\n",
    "            if i in self.x_pos:\n",
    "                mat[coord[i][0]][coord[i][1]] = 'x'\n",
    "            elif i in self.o_pos:\n",
    "                mat[coord[i][0]][coord[i][1]] = 'o'\n",
    "        res = \"\"\n",
    "        for i in range(len(mat)):\n",
    "            for j in range(len(mat[0])):\n",
    "                res += mat[i][j]\n",
    "            res += \"\\n\"\n",
    "        print(res)\n",
    "\n",
    "    def evaluate_board(self, tup):\n",
    "        x, o = tup\n",
    "        self.x_pos = set(x)\n",
    "        self.o_pos = set(o)\n",
    "        moves_list = []\n",
    "        for i in range(1, 10):\n",
    "            if i not in self.x_pos and i not in self.o_pos:\n",
    "                moves_list.append(i)\n",
    "        self.moves = set(moves_list)\n",
    "        _, res = self.check_victory()\n",
    "        return res\n",
    "        \n",
    "\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Agents\n",
    "We start by creating a random agent, that plays a random move from the available ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x wins\n",
      "xxo\n",
      "oxo\n",
      "xox\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def random_agent(game):\n",
    "    return choice(tuple(game.moves))\n",
    "\n",
    "game = TicTacToe()\n",
    "while not game.check_victory()[0]:\n",
    "    game.play(random_agent(game))\n",
    "print(game.check_victory()[0])\n",
    "game.board()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Decision Problem\n",
    "We can model the problem as a Markov Decision Problem, to do so, we must define some key elements:\n",
    "- **States**: they represent the possible list of states that our agent might have to face, in our case it's the list of board states in which it's our turn;\n",
    "- **Actions**: this is a list of all the possible moves we can employ, in this case the list of empty squares in a state;\n",
    "- **Transition model**: it's the way to get a new state from a previous one and a given action, in this case we expect that after making a move a different random empty space is filled by the opponent;\n",
    "- **Rewards**: these should be the rewards to give our agent when a certain breakthrough is reached for example 1 for winning and -1 for losing and -0.5 for tying;\n",
    "- **Discount**: it's an hyperparameter that influences how much our agent prefers instant rewards over delayed ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utility functions, the mdp implementation is in the next cell\n",
    "\n",
    "def list_to_state(myList): #given a list or a tuple, it splits it in half and transforms it into a string\n",
    "    if isinstance(myList, list):\n",
    "        cut = ceil(len(myList)/2)\n",
    "        x = myList[:cut]\n",
    "        if x is None:\n",
    "            x = []\n",
    "        o = myList[cut:]\n",
    "        if o is None:\n",
    "            o = []\n",
    "        x.sort()\n",
    "        o.sort()\n",
    "    elif isinstance(myList, tuple):\n",
    "        x, o = myList\n",
    "        if x is None:\n",
    "            x = []\n",
    "        if o is None:\n",
    "            o = []\n",
    "    else:\n",
    "        assert('Wrong type of variable passed to list_to_state')\n",
    "    xstring = ' '.join(map(lambda e: str(e), x))\n",
    "    ostring = ' '.join(map(lambda e: str(e), o))\n",
    "    return xstring+','+ostring\n",
    "\n",
    "def state_to_tuple(state): #converts a string back to a tuple containing the positions of xs and os\n",
    "    halves = state.split(',')\n",
    "    x = list(map(lambda e: int(e), halves[0].split()))\n",
    "    o = list(map(lambda e: int(e), halves[1].split()))\n",
    "    return tuple((x, o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mdp:\n",
    "    def __init__(self, symbol='x'):\n",
    "        self.symbol = symbol # this is the symbol we are playing as\n",
    "        self.states = self.generate_states()\n",
    "        self.discount = 1\n",
    "\n",
    "    def generate_states(self): #generate all states in which is our turn\n",
    "        all_states = list()\n",
    "        squares = range(1, 10)\n",
    "        for p in permutations(squares, 9):\n",
    "            for taken in range(10):\n",
    "                all_states.append(p[:taken])\n",
    "        actual_states = set()\n",
    "        evaluator = TicTacToe()\n",
    "        for s in tqdm(all_states):\n",
    "            if(len(s)%2 == 0  and self.symbol == 'x') or (len(s)%2 == 1 and self.symbol == 'o'): #only states in which is our turn\n",
    "                state = list_to_state(list(s))\n",
    "                evaluation = evaluator.evaluate_board(state_to_tuple(state))\n",
    "                if evaluation != 3: #ignore invalid states\n",
    "                    actual_states.add(state)\n",
    "        return actual_states\n",
    "      \n",
    "\n",
    "    def generate_actions(self, state):\n",
    "        actions = set()\n",
    "        x, o = state_to_tuple(state)\n",
    "        for move in range(1, 10):\n",
    "            if move not in x and move not in o:\n",
    "                actions.add(move)\n",
    "        return actions\n",
    "\n",
    "    def reward(self, state):\n",
    "        evaluator = TicTacToe()\n",
    "        winner = evaluator.evaluate_board(state_to_tuple(state))\n",
    "        if winner == 0:\n",
    "            return 0\n",
    "        if (winner==1 and self.symbol=='x') or (winner==2 and self.symbol=='o'):\n",
    "            return 1\n",
    "        elif winner==4:\n",
    "            return -0.5\n",
    "        else:\n",
    "            return -1\n",
    "    \n",
    "    def transition(self, state, actions, action): #returns the possible following states given a starting one and a move\n",
    "        x, o = state_to_tuple(state)\n",
    "        evaluator = TicTacToe()\n",
    "        enemy = actions.copy()\n",
    "        enemy.remove(action)\n",
    "        if evaluator.evaluate_board(state_to_tuple(state)) != 0: #check if state is terminal\n",
    "            return [state], 0 #return back the state and set the coefficient to 0\n",
    "        following = []\n",
    "        if self.symbol == 'x':\n",
    "            x.append(action)\n",
    "            x.sort()\n",
    "        else:\n",
    "            o.append(action)\n",
    "            o.sort()\n",
    "        if evaluator.evaluate_board(tuple((x, o))) != 0: #check if state is terminal\n",
    "            following.append(list_to_state(tuple((x, o))))\n",
    "        if len(following)!=0:\n",
    "            return following, 1\n",
    "        base_x = x.copy()\n",
    "        base_o = o.copy()\n",
    "        for e in enemy:\n",
    "            x = base_x.copy()\n",
    "            o = base_o.copy()\n",
    "            if self.symbol == 'x':\n",
    "                o.append(e)\n",
    "                o.sort()\n",
    "            else:\n",
    "                x.append(e)\n",
    "                x.sort()\n",
    "            if evaluator.evaluate_board(tuple((x, o))) != 3:      #discard impossible states\n",
    "                following.append(list_to_state(tuple((x, o))))\n",
    "        return following, 1/len(following)\n",
    "\n",
    "            \n",
    "        \n",
    "    def q_value(self, state, utility):\n",
    "        actions = self.generate_actions(state)\n",
    "        res = []\n",
    "        for a in actions:\n",
    "            possible, prob = self.transition(state, actions, a)\n",
    "            v=0\n",
    "            for p in possible:\n",
    "                try:\n",
    "                    v += prob * (self.reward(p) + self.discount*utility[p][0])\n",
    "                except KeyError:        #this should happen if the move found is terminal\n",
    "                    v += self.reward(p)\n",
    "            res.append(tuple((v, a)))\n",
    "        if len(res)==0:\n",
    "            res.append(tuple((self.reward(state), None)))\n",
    "        return res\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman Equation\n",
    "We want to find an agent able to perform the optimal policy, that is, a way to choose a good move in a given state. A good way to estimate such a policy is to iteratively calculate the Bellman Equation for each state, that is:\n",
    "$$U(s) = max_{a\\in A(s)}\\sum_{s'}P(s'|s, a)[R(s, a, s')+\\gamma U(s')]$$\n",
    "Where:\n",
    "- $a \\in A(s)$ denotes the possible actions in state $s$;\n",
    "- $s'$ is one of the possible states generated by each action;\n",
    "- $P(s'|s, a)$ is the probability of reaching state $s'$ performing action $a$ in state $s$;\n",
    "- $R(s, a, s')$ is the reward calculated on the transition, _in this case it only depends on the resulting state $s'$ unless $s$ is already a terminal state_;\n",
    "- $\\gamma$ is the discount parameter defined in `mdp.discount`;\n",
    "- $U(s)$ is the utility associated to each state, notice that with each iteration the utility of each state becomes more accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(problem:TicTacToe, epsilon):\n",
    "    utilities_prime = dict()\n",
    "    print(\"threshold:\" ,str(epsilon * (1 - problem.discount)/problem.discount))\n",
    "    epoch=0\n",
    "    for s in problem.states:\n",
    "        utilities_prime[s] = tuple((0, 0))\n",
    "    while True:\n",
    "        delta = 0\n",
    "        epoch += 1\n",
    "        utilities = utilities_prime.copy()\n",
    "        for s in tqdm(problem.states):\n",
    "            utilities_prime[s] = max(problem.q_value(s, utilities), key = lambda x: x[0])\n",
    "            delta = max(delta, abs(utilities_prime[s][0] - utilities[s][0]))\n",
    "        print('epoch:', epoch, 'delta:', delta)\n",
    "        if delta <= epsilon * (1 - problem.discount)/problem.discount:\n",
    "            break\n",
    "    return utilities_prime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function should be able to return a _dictionary_ containing, for each state, the evaluation of how \"good\" it is and the best move to use in that case. We can try it against the random agent and see if it's able to win."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utility function\n",
    "def game_to_state(game:TicTacToe):\n",
    "    x = list(game.x_pos)\n",
    "    o = list(game.o_pos)\n",
    "    x.sort()\n",
    "    o.sort()\n",
    "    return list_to_state(tuple((x, o)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3628800/3628800 [00:09<00:00, 379434.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3055/3055 [00:00<00:00, 8848.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 delta: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3055/3055 [00:00<00:00, 8918.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 delta: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3055/3055 [00:00<00:00, 8479.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 delta: 0.8333333333333331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3055/3055 [00:00<00:00, 8851.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 delta: 0.16145833333333348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3055/3055 [00:00<00:00, 8549.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 delta: 0.0026041666666667407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3055/3055 [00:00<00:00, 8041.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 delta: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3628800/3628800 [00:10<00:00, 331432.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2835/2835 [00:00<00:00, 9180.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 delta: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2835/2835 [00:00<00:00, 9415.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 delta: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2835/2835 [00:00<00:00, 9148.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 delta: 0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2835/2835 [00:00<00:00, 8166.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 delta: 0.16666666666666669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2835/2835 [00:00<00:00, 8495.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 delta: 0.033333333333333326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2835/2835 [00:00<00:00, 8595.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 delta: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#These are the dictionaries containing the policies\n",
    "policy_x = value_iteration(mdp(symbol='x'), 0.0001)\n",
    "policy_o = value_iteration(mdp(symbol='o'), 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The agent has a winrate of 99.41% when playing X against a random opponent\n",
      "The agent has a winrate of 93.81% when playing O against a random opponent\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#evaluate policy for X\n",
    "wins = 0\n",
    "for _ in range(10_000):\n",
    "    game = TicTacToe()\n",
    "    while not game.check_victory()[0]:\n",
    "        if game.next == 'x':\n",
    "            game.play(policy_x[game_to_state(game)][1])\n",
    "        else:\n",
    "            game.play(random_agent(game))\n",
    "    if game.check_victory()[1] == 1:\n",
    "        wins += 1\n",
    "print('The agent has a winrate of ' + str(wins/100) +'% when playing X against a random opponent')\n",
    "\n",
    "#evaluate policy for O\n",
    "wins = 0\n",
    "for _ in range(10_000):\n",
    "    game = TicTacToe()\n",
    "    while not game.check_victory()[0]:\n",
    "        if game.next == 'o':\n",
    "            game.play(policy_o[game_to_state(game)][1])\n",
    "        else:\n",
    "            game.play(random_agent(game))\n",
    "    if game.check_victory()[1] == 2:\n",
    "        wins += 1\n",
    "print('The agent has a winrate of ' + str(wins/100) +'% when playing O against a random opponent')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try it yourself\n",
    "\n",
    "| 2     | 9     | 4     |\n",
    "|-------|-------|-------|\n",
    "| **7** | **5** | **3** |\n",
    "| **6** | **1** | **8** |\n",
    "\n",
    "Here you can try to challenge the agent at tic tac toe. (fancy interface not included)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are playing as x\n",
      "---\n",
      "---\n",
      "---\n",
      "\n",
      "--o\n",
      "---\n",
      "-x-\n",
      "\n",
      "x-o\n",
      "--o\n",
      "-x-\n",
      "\n",
      "x-o\n",
      "-xo\n",
      "-xo\n",
      "\n",
      "o wins\n"
     ]
    }
   ],
   "source": [
    "s = None\n",
    "game = TicTacToe()\n",
    "while s != 'x' and s!='o':\n",
    "    s = input('Choose which symbol to play as (x or o):')\n",
    "\n",
    "print('You are playing as '+ s)\n",
    "\n",
    "while not game.check_victory()[0]:\n",
    "    if game.next==s:\n",
    "        game.board()\n",
    "        a = 0\n",
    "        while not game.play(int(a)):\n",
    "            a = input('Choose which move to play, refer to the table above:')\n",
    "    elif game.next=='o':\n",
    "        game.play(policy_o[game_to_state(game)][1])\n",
    "    elif game.next=='x':\n",
    "        game.play(policy_x[game_to_state(game)][1])\n",
    "game.board()\n",
    "print(game.check_victory()[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "[Giovanni Squillero](https://github.com/squillero/computational-intelligence) for the original code<br>\n",
    "[Artificial Intelligence: A Modern Approach 4th Edition by Russel, Norvig](https://www.google.it/books/edition/Artificial_Intelligence_A_Modern_Approac/cb0qEAAAQBAJ?hl=it) to implement Reinforcement Learning and the Bellman Equation <br>\n",
    "[This medium article](https://medium.com/@nour.oulad.moussa/tic-tac-toe-with-reinforcement-learning-part-i-markov-decision-process-value-policy-iteration-c4bcbb0b9fbe) for some ideas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ci-fLJ3OwGs-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
